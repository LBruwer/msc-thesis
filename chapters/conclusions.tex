% chapter Conclusions

Grid monitoring is an important factor when working with Grid Systems. Reports extracted from monitoring systems, support decisions for capacity management, and prove that a system meets requirements needed for Service Level Agreements.

Metrics for Computing Element performance monitoring usually are:

\begin{enumerate}
 \item {\bf Total Jobs} in running or waiting to run state.
 \item Individual per working node {\bf Processor Load}.
 \item {\bf Benchmarking} metrics such as FLOP/s.
\end{enumerate}

This project focused in the calculation, aggregation and transfer to present the metrics for Processor Load of working nodes of a Computing Element. Running and waiting jobs monitoring is extremely analyzed under availability monitoring research.

Calculation using the number of processes waiting in the queue of kernel scheduler were used. It is explained why this metric is more accurate than the classic percentage CPU usage.

Aggregation and transfer of metrics where examined in many different levels, from the multicasted XDR of Gmond to the resource providers of the information service that were used.

Information Service is core technology that used in Grid Computing. It has evolved in parallel with Middleware. Current version of the Monitoring and Discovery Service standard has reached the MDS4, introducing the use of Web Services. MDS2 was based in LDAP, which is still used by some systems to discover services.

Nagios bulk aggregation features using NPCD and MSG-Nagios-Bridge also provide a method to aggregate Ganglia metrics without the use of the MDS.

Presentation were simply examined using:

\begin{enumerate}
 \item {\bf DOM technology} to parse XML taken from WSRF through WebMDS interface using XPath
 \item simple {\bf LDAP} communication to take the metrics from the BDII Information Service
\end{enumerate}

\section{Conclusions}

Every aspect of grid monitoring keeps {\bf scalability in mind}. Distribution of metrics in all worker nodes using Gmond is the key to keep redundant that information.

BDII Information Service is great for use in site level environment, as LDAP performs faster than WSRF in less than a hundred users.

Site monitoring needs Nagios and Ganglia for a few hundreds of nodes. Nagios web interface is enough for site-level host and service view. Ganglia web interface supports good aggregation of many clusters, to the Region-level.

WSRF caching feature, Indexer and Aggregator Framework scales better and may be used for regional and top level monitoring.

Heterogeneous environments should {\bf rely on standards} in order to inter-operate and stay reliable. Even the Information Services mentioned above stick to the Glue schema.

A tool may seem to be great for its purpose, but after focusing on a need is may be discontinued, such as SAM. This example is taken from the availability monitoring, as its frontend was replaced by MyEGI and Nagios keeping its back-end in MRS.

\section{Further Work}

\subsection{Aggregation}

Additional investigation is required to be carried out for the aggregation of collected information. WSRF offer the Aggregator Framework, which downstream information from many EPRs and deliver it through the Index service. Scaling such information in the Regional monitoring level may introduce issues that only in that scale will be visible.

\subsection{Benchmarking}

Performance metrics which are produced using benchmarking software is a good standard way of measuring and advertising the performance of Computing Elements. Such metrics are not efficient for regularly runs and frequent changes because they are cost effective performance wise. It is good although to present the performance ability of a CE. Rare periodic execution and push in the information service, will offer a good metric to select one CE of another.

\subsection{Storage element performance monitoring}

Storage Element performance monitoring is an interesting area for research. Vendors that produce enterprise level storage systems also offer tools to monitor the performance of the storage system. The most used metric is I/O Operations per second and throughput in GByte per second, in every aspect of a storage system.

Storage systems are also provided by different vendors and may also be deployed as custom solution. Fiber Channel solutions versus Clustered File System cases are examined \cite{brzezniak2008analysis} as a chapter of distributed systems, storage oriented.

Hadoop is the most commonly used distributed data storage. In CMS, an experiment that is known for its huge needs in data storage, Hadoop has been adopted \cite{hadoop} as the distributed file system. Grid computing community has adopted the use of Total and Used GB per Storage Element (as in Computing Element), but there is still enough work on the performance monitoring.

