\section{Methods Adopted}

In complex distributed systems such as grids, performance bottlenecks may be located using monitoring data. From the processor usage on a single node of a computing element to the total usage of processed jobs in a large cluster, performance data help to focus on the problem that impacts the overall performance.

In order to succeed in grid monitoring, some requirements should be considered. A very large amount of data should be delivered real-time, from many heterogeneous sources on different networks or even countries. These data must be accurate and consistent. There should be synchronized timestamps on the generation of each metric, to the measurement value that should be comparable between different architectures. The time synchronization of the hosts of each cluster may be done using network time protocol, so all metrics are taken on the time that they actually report. Metrics should have error bounds to preserve accuracy, and the consistency issue is solved using coordination of that activity, so the impact of a metric to other sensors is controlled.

The flow of the monitoring process initialization is described from the GMA standard. The application-consumer queries the directory service in order to declare its interest to get metrics for a specific host/cluster. The sensors of the elements that is equivalent to the specific query generates the metrics that will be given to the consumer from the producer, which in turn queries the directory service to find the consumer. The producer is the one that initializes the connection to the consumer in order to deliver the measurements, even if the consumer had asked the directory service for this. \cite{balatonuse}


\subsection{Performance Metrics}

CALC\_LOAD - load average

extra
\subsubsection{Transport and sample}
Gmond code uses the ganglia libmetrics library which in case of Linux operating system parses the $/proc/loadavg$ pseudo-file to get linux kernel calculated system load average.

\begin{lstlisting}[language=C,caption=libmetrics code to get load average]
timely_file proc_loadavg = { {0,0} , 5., "/proc/loadavg" };
/* ... */
g_val_t
load_one_func ( void )
{
   g_val_t val;
   val.f = strtod( update_file(&proc_loadavg), (char **)NULL);
   return val;
}
\end{lstlisting}
code from gmond client

\subsection{Information Systems}

\subsubsection{BDII}

\subsubsection{WSRF}

The XML that Ganglia Resource Provider took from Gmond process through TCP, using XSLT technology is transformed on WSRF to another XML document, that is following the Glue-CE schema. In directory globus\_wsrf\_mds\_usefulrp of globus configuration root, there is the file ganglia\_to\_glue.xslt where we can focus on the transformation rules. A snippet of interest for the case of ProcessorLoad class is seen in Listing \ref{xslt}.

\begin{lstlisting}[language=XML,caption=WSRF XSLT for Ganglia Information Provider,label=xslt]
<glue:ProcessorLoad>

<xsl:attribute name="glue:Last1Min">
  <xsl:call-template name="emitProperNumeric">
    <xsl:with-param name="numeric" 
    select="floor(100 * METRIC[@NAME='load_one']/@VAL)"/>
  </xsl:call-template>
</xsl:attribute>

<xsl:attribute name="glue:Last5Min">
  <xsl:call-template name="emitProperNumeric">
    <xsl:with-param name="numeric" 
    select="floor(100 * METRIC[@NAME='load_five']/@VAL)"/>
  </xsl:call-template>
</xsl:attribute>

<xsl:attribute name="glue:Last15Min">
  <xsl:call-template name="emitProperNumeric">
    <xsl:with-param name="numeric" 
    select="floor(100 * METRIC[@NAME='load_fifteen']/@VAL)"/>
  </xsl:call-template>
</xsl:attribute>

</glue:ProcessorLoad>
\end{lstlisting}


\section{Interpretation of Results}


Discussion about performance results based on
load average.

some UNIX internals, processes, scheduler
not a percentage counter of CPU usage


Difference between these metrics and the availability of 
a grid based on the queue of jobs have been submitted.

Availability monitoring

-MyEGI, monitor visualization environment
-django data models
-MRS database, ATP based schema


\section{Specific Interpretations}

\subsection{Scaling}
\subsubsection{LDAP}

there is a paper about how information systems perform in large scale

LDAP as the core technology of MDS2 has been investigated \cite{zhang2004performance} and proved that scales and performs good when the data are kept in cache. The performance of the information system when it is accessed by a large number of cocurrent users have degrades dramatically when data caching is not used.

\subsubsection{WSRF}
deserialization of MDS query in gmond to WSRF

performance analysis of WSRF here \cite{schopf2006monitoring}

and MDS4 vs MDS2

\section{Enveloping Interpretations}
