% TODO na valw to gmond sto LTSP
laboratory

\section{Approach Adopted}

na perigrapsw to implementation of performance analysis of distributes systems
in GMA \cite{balatonuse}


\section{Design Methods}
\subsection{Recommendations and standards}
\subsubsection{Grid Monitoring Architecture}
\nomenclature{GMA}{Grid Monitoring Architecture}
%% TODO producer-consumer plot
\subsubsection{R-GMA}
%% TODO R-GMA literature
\nomenclature{R-GMA}{Relational Grid Monitoring Architecture}
\subsubsection{XML schema}
\subsubsection{GLUE}
gained wide acceptance given its adoption by Globus MDS3
\nomenclature{GLUE}{Grid Laboratory Uniform Environment}


\subsection{Information Infrastructure}
% TODO to MDS me dika mou logia
``Performance''.  The applications of interest to us frequently 
operate on  a  large scale  (e.g.,  hundreds  of  proces- 
sors) and have demanding performance requirements. 
Hence, an information infrastructure must permit rapid 
access to frequently used configuration information. It 
is not acceptable to contact  a  server  for every  item: 
caching is required.
``Scalability and cost''. The infrastructure must scale to large
numbers of components and permit concurrent access
by many entities. At the same time, its organization
must permit easy discovery of information. The human
and resource costs (CPU cycles, disk space, network
bandwidth) of creating and maintaining information
must also be low, both at individual sites and in total.
``Uniformity''. Our goal is to simplify the development of
tools and applications that use data to guide configuration
decisions. We require a uniform data model
as well as an application programming interface (MI)
for common operations on the data represented via that
model. One aspect of this uniformity is a standard representation
for data about common resources, such as
processors and networks.
``The X.500 standard'' defines a directory service
that can be used to provide extensible distributed directory
services within a wide area environment. A directory service
is a service that provides read-optimized access to general
data about entities, such as people, corporations, and computers.
X.500 provides a framework that could, in principle,
be used to organize the information that is of interest to us.
\cite{mds1}

\section{Data-acquisition Systems}
plugins that take metrics (SAM) and send results to nagios:

https://twiki.cern.ch/twiki/bin/view/LCG/SAMProbesMetrics

http://nationalgridservice.blogspot.com/2010/10/nagios-myegee-and-myegi.html

4 layers to performance investigation:
\begin{enumerate}
  \item Storage elements
  \item Sites
  \item VOs
  \item Middleware
\end{enumerate}
3 benchmarking categories
\begin{enumerate}
  \item micro-benchmarks
  \item micro-kernels
  \item application kernels
\end{enumerate}
Benchmarking

HPL
\cite{gridbench}

CE performance
free processors
MFLOPS
MIPS (instructions per second)
free RAM

SE performance
IOPS
free space
\subsection{Ganglia}

{\bf CPU load} is taken using the pseudo /proc/loadavg file which in turn is
filled by Linux kernel's CALC\_LOAD macro. It represents the number of processes
that are waiting on the run queue, for three intervals of $1$, $5$ and $15$ minutes.

\subsection{pnp4nagios}

\section{Range of cases examined}
\subsection{Metrics}
% here there will be an enumeration of metrics, gmond, etc

Grid performance can be measured using benchmark tools in different levels of
the grid architecture, using the micro-benchmarks at the Worker Node level, the
Site (CE) level and the Grid VO level. Various benchmarks exist in these
levels, using different libraries and algorithms, such as  This project focuses
on mathematically compute of the performance of a grid based on the metrics that
are taken at the Worker Node level.

Different metrics and benchmarks exist, such as the measurement of
the performance of CPUs in {\bf MIPS using EPWhetstone} and the evaluation of
the performance of a CPU in {\bf FLOP/s and MB/s using BlasBench}. GridBench
\cite{gridbench} provides a framework to collect those metrics using its own
description language, {\bf GBDL}.

GcpSensor \cite{gcpsensor} introduce a new performance metric called WMFLOPS. It
uses PAPI \cite{papi} (Performance API) to access the hardware performance
counters. For data distribution it uses MDS information system which provides
dynamic metrics for CPU load average, one for 1, for 5 and for 15 minutes load.

% TODO ganglia to MDS
\subsection{Publish to Information System}

\subsubsection{LDAP based - MDS/BDII}
\begin{enumerate}
  \item Python ganglia client script:
  http://globus.org/toolkit/docs/2.4/mds/gangliaprovider.html
  \item Perl gaglia-IP tool:
  
  http://www.star.bnl.gov/public/comp/Grid/Monitoring/SimpleGangliaIP.html
  
  http://www.star.bnl.gov/public/comp/Grid/Monitoring/ganglia\_ip

\end{enumerate}

Glue schema

GlueCluster
GlueClusterTop
GlueGeneralTop
GlueHost
GlueHostArchitecture
GlueHostMainMemory
GlueHostNetworkAdapter
GlueHostOperatingSystem
GlueHostProcessor
GlueHostProcessorLoad
GlueHostSMPLoad
GlueSubCluster
GlueTop

Perl:
\begin{lstlisting}
$[root@mon ~]# ./ganglia_ip -h mon -p 8649 -o mds
\end{lstlisting}

Python:
\begin{lstlisting}
$[root@mon ~]# /opt/ganglia/bin/ganglia --format=MDS
\end{lstlisting}


\subsubsection{Web Service based - WSRF}
using WSRF (GT4, information services, information providers)
\url{http://index.training.cagrid.org:8080/wsrf/services/DefaultIndexService?wsdl}

